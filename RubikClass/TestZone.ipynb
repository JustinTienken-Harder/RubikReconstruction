{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from StickerEnsemble import EnsembleStickerCube\n",
    "\n",
    "\n",
    "class Environment(EnsembleStickerCube):\n",
    "    \"\"\"\n",
    "    CUBE ENVIRONMENT\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__(randomize_representation = True)\n",
    "        self.valid_turns = [\"U\", \"U'\", \"R\", \"R'\", \"L\", \"L'\",\n",
    "                       \"F\", \"F'\", \"B\", \"B'\", \"D\", \"D'\"]\n",
    "        self.turns_thusfar = 0\n",
    "        \n",
    "    def make_start_state(self, number):\n",
    "        \"\"\"Resets the cubes. Generate a 'random' scramble. Return the image.\"\"\"\n",
    "        self.reset()\n",
    "        turn_arr = list(np.random.choice(self.valid_turns, size = number))\n",
    "        turns = \" \".join(turn_arr)\n",
    "        self.__call__(turns)\n",
    "        return self.visualize()\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        cube = self.cubes[0]\n",
    "        sticker_list = cube.current_state\n",
    "        reward = 0\n",
    "        done = cube.is_solved()\n",
    "        if done:\n",
    "            reward += 20\n",
    "        for i in range(6):\n",
    "            side = sticker_list[(i*9):((i+1)*9)]\n",
    "            count = dict()\n",
    "            for stick in side:\n",
    "                if stick in count:\n",
    "                    count[stick]+=1\n",
    "                else:\n",
    "                    count[stick] = 1\n",
    "            reward += max((y for x, y in count.items()))\n",
    "        return reward, done\n",
    "    \n",
    "    def state_and_reward(self, current_state, picked_action):\n",
    "        \"\"\"\n",
    "        Should take the current state and the action and return the new state and the reward.\n",
    "        \"\"\"\n",
    "        self.turns_thusfar += 1\n",
    "            \n",
    "        actual_action = self.valid_turns[picked_action]\n",
    "        self.__call__(actual_action)\n",
    "        reward, done = self._get_reward()\n",
    "        if self.turns_thusfar == 25:\n",
    "            print(\"HALFIES\")\n",
    "        if self.turns_thusfar == 50 or done:\n",
    "            done = True\n",
    "            self.turns_thusfar = 0\n",
    "        return self.visualize(), reward, done\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DPN(nn.Module):\n",
    "    \"\"\"AGENTS\"\"\"\n",
    "    def __init__(self, alpha, input_size, output_size):\n",
    "        super(DPN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        self.fc5 = nn.Linear(input_size, output_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = T.tensor(x)\n",
    "        residual = x\n",
    "        h = F.leaky_relu(self.fc1(x)) + x\n",
    "        h = F.leaky_relu(self.fc2(h)) + h + x\n",
    "        h = F.leaky_relu(self.fc3(h)) +h\n",
    "        h = F.leaky_relu(self.fc4(h)) +h\n",
    "        h = F.softmax(self.fc5(h))\n",
    "        return h\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"CRITIC\"\"\"\n",
    "    def __init__(self, beta, input_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.fc7 = nn.Linear(input_size, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta, weight_decay=0.1)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = T.tensor(x)\n",
    "        h = F.leaky_relu(self.fc1(x))\n",
    "        h = F.leaky_relu(self.fc2(h))\n",
    "        out = self.fc7(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims,output_dims, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = None\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.actor = DPN(alpha, input_dims,output_dims)\n",
    "        self.critic = Critic(beta, input_dims)\n",
    "        self.actor.apply(weights_init_uniform_rule)\n",
    "        self.critic.apply(weights_init_uniform_rule)\n",
    "        self.critic_target = Critic(beta, input_dims)\n",
    "        self.update_critic_target()\n",
    "    \n",
    "    def update_critic_target(self):\n",
    "        self.critic_target.load_state_dict(self.critic_target.state_dict())\n",
    "\n",
    "    def fix_obs(self, observation):\n",
    "        observation = observation.flatten()\n",
    "        observation = observation/255\n",
    "        observation = T.from_numpy(observation.astype(np.float)).float()\n",
    "        return observation\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        observation = self.fix_obs(observation)\n",
    "        probs  = self.actor.forward(observation)#.to(self.actor.device)\n",
    "        action_probs = Categorical(probs)\n",
    "        action = action_probs.sample()\n",
    "        self.log_probs = action_probs.log_prob(action).to(self.actor.device)\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        state = self.fix_obs(state)\n",
    "        new_state = self.fix_obs(new_state)\n",
    "        #self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "\n",
    "        critic_value_ = self.critic.forward(new_state)\n",
    "        critic_value = self.critic.forward(state)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "        delta = ((reward + -self.gamma*critic_value_*(1-int(done))) + \\\n",
    "                                                                critic_value)\n",
    "\n",
    "        critic_loss = delta**2\n",
    "        critic_loss.backward(retain_graph = True)\n",
    "        #(actor_loss + critic_loss).backward()\n",
    "        #T.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.01)\n",
    "        #print(critic_value)\n",
    "        self.critic.optimizer.step()\n",
    "        actor_loss = -1*self.log_probs * delta.detach()\n",
    "        #actor_loss.backward()\n",
    "        #T.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.01)\n",
    "        #self.actor.optimizer.step()\n",
    "        return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exiz\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\Exiz\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALFIES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exiz\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Exiz\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 874.0 average score 874.0     avg single score:  29.133333333333333  test:  nan\n",
      "HALFIES\n",
      "episode  1 score 1063.0 average score 968.5     avg single score:  35.43333333333333  test:  nan\n",
      "HALFIES\n",
      "episode  2 score 850.0 average score 929.0     avg single score:  28.333333333333332  test:  nan\n",
      "HALFIES\n",
      "episode  3 score 962.0 average score 937.2     avg single score:  32.06666666666667  test:  nan\n",
      "HALFIES\n",
      "episode  4 score 825.0 average score 914.8     avg single score:  27.5  test:  nan\n",
      "HALFIES\n",
      "episode  5 score 949.0 average score 920.5     avg single score:  31.633333333333333  test:  nan\n",
      "HALFIES\n",
      "episode  6 score 1089.0 average score 944.6     avg single score:  36.3  test:  nan\n",
      "HALFIES\n",
      "episode  7 score 886.0 average score 937.2     avg single score:  29.533333333333335  test:  nan\n",
      "HALFIES\n",
      "episode  8 score 876.0 average score 930.4     avg single score:  29.2  test:  nan\n",
      "HALFIES\n",
      "episode  9 score 902.0 average score 927.6     avg single score:  30.066666666666666  test:  nan\n",
      "HALFIES\n",
      "episode  10 score 811.0 average score 917.0     avg single score:  27.033333333333335  test:  nan\n",
      "HALFIES\n",
      "episode  11 score 948.0 average score 919.6     avg single score:  31.6  test:  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3ebce4380a10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_and_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mactor_total_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-45a22f4baf45>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, state, reward, new_state, done)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#T.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.01)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m#print(critic_value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m#actor_loss.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                    )\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(alpha=0.3,beta=0.3, input_dims = 3888, output_dims = len([\"U\", \"U'\", \"R\", \"R'\", \"L\", \"L'\",\n",
    "                       \"F\", \"F'\", \"B\", \"B'\", \"D\", \"D'\"]))\n",
    "\n",
    "n_games = 50000\n",
    "run_name = \"please_finish_by_morning\"\n",
    "import os\n",
    "if not os.path.exists(run_name):\n",
    "    os.makedirs(run_name)\n",
    "#print(agent.actor.state_dict())\n",
    "\n",
    "x = [int(round(x,0)) for x in np.linspace(0,25, ngames)+1]\n",
    "\n",
    "run_name = \"FAST\"\n",
    "import os\n",
    "if not os.path.exists(run_name):\n",
    "    os.makedirs(run_name)\n",
    "\n",
    "scores = []\n",
    "for i in x:\n",
    "    done = False\n",
    "    observation = env.make_start_state(i)\n",
    "    score = 0\n",
    "    agent.actor.optimizer.zero_grad()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done = env.state_and_reward(observation ,action)\n",
    "        actor_loss = agent.learn(observation, reward, observation_, done)\n",
    "        if score == 0:\n",
    "            actor_total_loss = actor_loss\n",
    "        else:\n",
    "            actor_total_loss += actor_loss\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "    actor_loss = actor_loss\n",
    "    actor_loss.backward()\n",
    "    agent.actor.optimizer.step()\n",
    "    if i%20 == 0:\n",
    "        agent.update_critic_target()\n",
    "\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            avg = np.mean(scores[-1000:])\n",
    "            location = \"./\"+run_name+ \"/\"+str(avg)+\"_avg_\"+str(i)+\"_ngames.pt\"\n",
    "            torch.save(agent.actor.state_dict(), location)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode ', i, 'score %.1f' % score,\n",
    "            'average score %.1f' % avg_score, \"    avg single score: \", score/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i+1 for i in range(n_games-100)]\n",
    "plt.plot(scores)\n",
    "location = run_name + \"/\" + run_name\n",
    "plt.savefig(location +\"train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = scores\n",
    "y2 = []\n",
    "mean = lambda x: sum(x)/len(x)\n",
    "for i in range(len(y1)):\n",
    "    if i < 100:\n",
    "        pass\n",
    "    else:\n",
    "        avg = mean(y1[(i-100):i])\n",
    "        y2.append(avg)\n",
    "        \n",
    "plt.plot(y2)\n",
    "#plt.show()\n",
    "plt.savefig(location+\"train_smooth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
