{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from StickerEnsemble import EnsembleStickerCube\n",
    "\n",
    "\n",
    "class Environment(EnsembleStickerCube):\n",
    "    \"\"\"\n",
    "    CUBE ENVIRONMENT\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__(randomize_representation = True)\n",
    "        self.valid_turns = [\"U\", \"U'\", \"R\", \"R'\", \"L\", \"L'\",\n",
    "                       \"F\", \"F'\", \"B\", \"B'\", \"D\", \"D'\"]\n",
    "        self.turns_thusfar = 0\n",
    "        \n",
    "    def make_start_state(self, number):\n",
    "        \"\"\"Resets the cubes. Generate a 'random' scramble. Return the image.\"\"\"\n",
    "        self.reset()\n",
    "        turn_arr = list(np.random.choice(self.valid_turns, size = number))\n",
    "        turns = \" \".join(turn_arr)\n",
    "        self.__call__(turns)\n",
    "        return self.visualize()\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        cube = self.cubes[0]\n",
    "        sticker_list = cube.current_state\n",
    "        reward = 0\n",
    "        done = cube.is_solved()\n",
    "        if done:\n",
    "            reward += 20\n",
    "        for i in range(6):\n",
    "            side = sticker_list[(i*9):((i+1)*9)]\n",
    "            count = dict()\n",
    "            for stick in side:\n",
    "                if stick in count:\n",
    "                    count[stick]+=1\n",
    "                else:\n",
    "                    count[stick] = 1\n",
    "            reward += max((y for x, y in count.items()))\n",
    "        return reward, done\n",
    "    \n",
    "    def state_and_reward(self, current_state, picked_action):\n",
    "        \"\"\"\n",
    "        Should take the current state and the action and return the new state and the reward.\n",
    "        \"\"\"\n",
    "        self.turns_thusfar += 1\n",
    "            \n",
    "        actual_action = self.valid_turns[picked_action]\n",
    "        self.__call__(actual_action)\n",
    "        reward, done = self._get_reward()\n",
    "        if self.turns_thusfar == 25:\n",
    "            print(\"HALFIES\")\n",
    "        if self.turns_thusfar == 50 or done:\n",
    "            done = True\n",
    "            self.turns_thusfar = 0\n",
    "        return self.visualize(), reward, done\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "TRAJECTORY_LENGTH = 30 #Approximately 10 seconds\n",
    "MIDPOINTS = 2 #splits video data into trajectories of length above, but this determines the amount of overlap across trajectories\n",
    "\n",
    "EPSILON_PERTURBATIONS = False  #if we want the network to predict how to perturb LS vector.\n",
    "ITERATIONS = 100 #kinda like epochs?\n",
    "BATCH_SIZE = 10   #Might be the exact same thing as episodes, up for interpretation.\n",
    "EPISODES = 20     #How many trajectories to explore for a given job. Essentually to get a better estimate of the expected reward.\n",
    "DISCOUNT = 0.99   #how much to discount the reward\n",
    "ALPHA = 0.001     #learning rate?\n",
    "INPUT_SIZE = 3888\n",
    "#weight = torch.Tensor([0.5])\n",
    "\n",
    "class DPN(nn.Module):\n",
    "    \"\"\"AGENTS\"\"\"\n",
    "    def __init__(self, alpha, input_size, output_size):\n",
    "        super(DPN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        self.fc5 = nn.Linear(input_size, output_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = T.tensor(x)\n",
    "        residual = x\n",
    "        h = F.leaky_relu(self.fc1(x)) + x\n",
    "        h = F.leaky_relu(self.fc2(h)) + h + x\n",
    "        h = F.leaky_relu(self.fc3(h)) +h\n",
    "        h = F.leaky_relu(self.fc4(h)) +h\n",
    "        h = F.softmax(self.fc5(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "#from torch.distributions.independent import Independent\n",
    "#from torch.distributions.normal import Normal\n",
    "import torch\n",
    "\n",
    "DATA_FILE_NAME = \"trajectory_dict.pickle\"\n",
    "TRAJECTORY_LENGTH = 30 #Approximately 10 seconds\n",
    "MIDPOINTS = 2 #splits video data into trajectories of length above, but this determines the amount of overlap across trajectories\n",
    "\n",
    "EPSILON_PERTURBATIONS = False  #if we want the network to predict how to perturb LS vector.\n",
    "ITERATIONS = 1000 #kinda like epochs?\n",
    "BATCH_SIZE = 10   #Might be the exact same thing as episodes, up for interpretation.\n",
    "EPISODES = 20     #How many trajectories to explore for a given job. Essentually to get a better estimate of the expected reward.\n",
    "DISCOUNT = 0.99   #how much to discount the reward\n",
    "ALPHA = 3e-3     #learning rate?\n",
    "\n",
    "def curried_valuation(length_of_longest_trajectory):\n",
    "    '''\n",
    "    Given the length of the longest trajectory of a set of episodes;\n",
    "    returns the function that will compute the valuation of an episode array (while padding it)\n",
    "    Result intended to be used as  map(valuation, episodes_array) to return valuation of each episodes.\n",
    "    '''\n",
    "    def valuation(episode):\n",
    "        '''\n",
    "        returns the valuation of an episode (with padding)\n",
    "        input: [(s_0, a_0, r_0), ... ,(s_t, a_t, r_t)]         potentially t<length_of_longest_trajectory\n",
    "        output: [v_0, v_1, ... v_L]\n",
    "        '''\n",
    "\n",
    "        length = len(episode)\n",
    "        if length != length_of_longest_trajectory:\n",
    "            #If the episode isn't as long as the longest trajectory, pad it\n",
    "            episode.extend([(0,0,0) for y in range(length_of_longest_trajectory-length)]) #have to make sure the numbers line up correctly\n",
    "        out = np.zeros(len(episode))\n",
    "        x = [i[2] for i in episode] #rewards\n",
    "        out[-1] = x[-1]\n",
    "        for i in reversed(range(len(x)-1)): #go backwards\n",
    "            out[i] = x[i] + DISCOUNT*out[i+1] #this step valuation = reward + gamma*next_step_valuation\n",
    "        #assert x.ndim >= 1\n",
    "        return out\n",
    "    return valuation\n",
    "\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "\n",
    "class DpnTraining:\n",
    "    def __init__(self, INPUT_SIZE, policy_net):\n",
    "        '''\n",
    "        INPUT_SIZE = size and shape from the environment's output for a state  TODO\n",
    "        OUTPUT_SIZE = number of possible actions                               TODO\n",
    "        Probably include stuff to interact with the environment after inputting a class\n",
    "        all caps words are hyperparameters you would set.\n",
    "        '''\n",
    "        self.env = Environment()\n",
    "\n",
    "        # Define the network\n",
    "        self.network = policy_net(ALPHA, 3888, 12)\n",
    "        self.network.apply(weights_init_uniform_rule)\n",
    "        # logging\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.rewards = []\n",
    "        self.variance = []\n",
    "        self.rewards_last = []\n",
    "        self.variance_last = []\n",
    "\n",
    "\n",
    "    def train(self, ITERATIONS):\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), lr = ALPHA) #This is roughly based on some pytorch examples. We use this to update weights of the model.\n",
    "        x = [int(round(x,0)) for x in np.linspace(0,25, ITERATIONS)+1]\n",
    "        cnt = 0 \n",
    "        for i in x:\n",
    "            cnt += 1 \n",
    "            first_frame = self.env.make_start_state(i) #this would be a list of starting states\n",
    "            jobs = [first_frame] #TODO: Coerce job variable to appropriate pytorch type. Necessary due to environment not set up to handle processing different trajectories.\n",
    "            self.train_on_jobs(jobs, optimizer)\n",
    "            print(\"Iteration \"+str(i+1)+\" Completed with reward: \" + str(self.rewards[-1]) + \" Variance of :\" + str(self.variance[-1]))\n",
    "            print(\"Average last avg reward: \" + str(self.rewards_last[-1]) + \" last variance avg: \" + str(self.variance_last[-1]))\n",
    "            if cnt % 100 == 0: \n",
    "                torch.save(self.network.state_dict(), location)\n",
    "\n",
    "\n",
    "\n",
    "    def fix_obs(self, observation):\n",
    "        observation = observation.flatten()\n",
    "        observation = observation/255\n",
    "        observation = T.from_numpy(observation.astype(np.float)).float()\n",
    "        return observation\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        The forward pass of the network on the given state. Returns the output probabilites for taking the OUTPUT_SIZE probabilites\n",
    "        might already be defined from the initialization after defining your model\n",
    "        '''\n",
    "        state = self.fix_obs(state)\n",
    "        probs = self.network(state)\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def trajectory(self, current_state):\n",
    "        '''\n",
    "        Maybe this implementation doesn't utilize GPUs very well, but I have no clue or not.\n",
    "        Final output looks like:\n",
    "        [(s_0, a_0, r_0), ..., (s_L, a_L, r_l)]\n",
    "        '''\n",
    "        output_history = []\n",
    "        while True:\n",
    "            probs = self.forward(current_state)\n",
    "            distribution = Categorical(probs)\n",
    "            picked_action = distribution.sample()\n",
    "            action = picked_action.detach()\n",
    "            #print(action)\n",
    "            new_state, reward, done = self.env.state_and_reward(current_state, action) #Get the reward and the new state that the action in the environment resulted in. None if action caused death. TODO build in environment\n",
    "            #Attempting this\n",
    "            lg = distribution.log_prob(action)\n",
    "            output_history.append( (current_state, action, reward, lg) )\n",
    "            if done: #essentially, you died or finished your trajectory\n",
    "                break\n",
    "            else:\n",
    "                current_state = new_state\n",
    "        return output_history\n",
    "\n",
    "    def train_on_jobs(self,jobset, optimizer):\n",
    "        '''\n",
    "        Training from a batch. Kinda presume the batch is a set of starting states not sure how you have the implemented states (do they include actions internally?)\n",
    "        example shape of episode_array\n",
    "        [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [1, 2, 3, 4, 5, 6, 7],\n",
    "        [1, 2, 3]\n",
    "        ]\n",
    "        '''\n",
    "        #optimizer.zero_grad()#Basically start gradient or how you'll change weights out at 0 but with the shape or whatever you need to update the weights through addition. TODO figure out how this thing should look\n",
    "        for job_start in jobset:\n",
    "            optimizer.zero_grad()\n",
    "            #episode_array is going to be an array of length N containing trajectories [(s_0, a_0, r_0), ..., (s_L, a_L, r_0)]\n",
    "            episode_array = [self.trajectory(job_start) for x in range(EPISODES)]\n",
    "            # Now we need to make the valuations\n",
    "            #temp\n",
    "            longest_trajectory = max(len(episode) for episode in episode_array)\n",
    "            valuation_fun = curried_valuation(longest_trajectory)\n",
    "            cum_values = np.array([valuation_fun(ep) for ep in episode_array]) #should be a EPISODESxlength sized\n",
    "            #Compute the baseline valuations.\n",
    "            baseline_array = np.array([sum(cum_values[:,i])/EPISODES for i in range(longest_trajectory)]) #Probably defeats the purpose of numpy, but we're essentially trying to sum each valuation array together, and then divide by the number of episodes\n",
    "            avg = baseline_array[0]\n",
    "            var = [np.sqrt(sum(np.square(cum_values[:,i]-baseline_array[i]))/EPISODES) for i in range(longest_trajectory)]\n",
    "            #log\n",
    "            self.rewards.append(avg)\n",
    "            self.variance.append(var[0])\n",
    "            self.variance_last.append(var[-1])\n",
    "            self.rewards_last.append(baseline_array[-1])\n",
    "            #policy updates\n",
    "            for i in range(EPISODES): #swapped two for loops\n",
    "                for t in range(longest_trajectory):\n",
    "                    try:\n",
    "                        state, action, reward, log_pro= episode_array[i][t]\n",
    "                    except IndexError: #this occurs when the trajectory is over.\n",
    "                        break\n",
    "                    if var[t] <1e-4:\n",
    "                        varry = 1e-4\n",
    "                    else:\n",
    "                        varry = var[t]\n",
    "                    #first two products are scalars, final is scalar multiplication of computed gradients on the NN\n",
    "                    if i ==0 and t == 0:\n",
    "                        loss = -(cum_values[i][t]-baseline_array[t])/(varry + self.eps) * log_pro #This is what it should look like in pytorch. Added negative on recommendation of pytorch documentation\n",
    "                    else:\n",
    "                        loss += -(cum_values[i][t]-baseline_array[t])/(varry + self.eps)*log_pro\n",
    "            loss.backward() #Compute the total cumulated gradient thusfar through our big-ole sum of losses\n",
    "            optimizer.step() #Actually update our network weights. The connection between loss and optimizer is \"behind the scenes\", but recall that it's dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13522\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-56ebf67fc8e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdpn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDpnTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3888\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDPN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-db79394870a3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, ITERATIONS)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mfirst_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_start_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#this would be a list of starting states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mjobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfirst_frame\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#TODO: Coerce job variable to appropriate pytorch type. Necessary due to environment not set up to handle processing different trajectories.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Completed with reward: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Variance of :\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average last avg reward: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards_last\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" last variance avg: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance_last\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-db79394870a3>\u001b[0m in \u001b[0;36mtrain_on_jobs\u001b[1;34m(self, jobset, optimizer)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m#episode_array is going to be an array of length N containing trajectories [(s_0, a_0, r_0), ..., (s_L, a_L, r_0)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m             \u001b[0mepisode_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m             \u001b[1;31m# Now we need to make the valuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;31m#temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-db79394870a3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m#episode_array is going to be an array of length N containing trajectories [(s_0, a_0, r_0), ..., (s_L, a_L, r_0)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m             \u001b[0mepisode_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m             \u001b[1;31m# Now we need to make the valuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;31m#temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-db79394870a3>\u001b[0m in \u001b[0;36mtrajectory\u001b[1;34m(self, current_state)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpicked_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;31m#print(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_and_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Get the reward and the new state that the action in the environment resulted in. None if action caused death. TODO build in environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;31m#Attempting this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0moutput_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "dpn = DpnTraining(INPUT_SIZE = 3888, policy_net = DPN)\n",
    "dpn.train(ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = Agent(alpha=0.3,beta=0.3, input_dims = 3888, output_dims = len([\"U\", \"U'\", \"R\", \"R'\", \"L\", \"L'\",\n",
    "                       \"F\", \"F'\", \"B\", \"B'\", \"D\", \"D'\"]))\n",
    "\n",
    "n_games = 50000\n",
    "run_name = \"please_finish_by_morning\"\n",
    "import os\n",
    "if not os.path.exists(run_name):\n",
    "    os.makedirs(run_name)\n",
    "#print(agent.actor.state_dict())\n",
    "\n",
    "x = [int(round(x,0)) for x in np.linspace(0,25, n_games)+1]\n",
    "\n",
    "run_name = \"FAST\"\n",
    "import os\n",
    "if not os.path.exists(run_name):\n",
    "    os.makedirs(run_name)\n",
    "\n",
    "scores = []\n",
    "for i in x:\n",
    "    done = False\n",
    "    observation = env.make_start_state(i)\n",
    "    score = 0\n",
    "    agent.actor.optimizer.zero_grad()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done = env.state_and_reward(observation ,action)\n",
    "        actor_loss = agent.learn(observation, reward, observation_, done)\n",
    "        if score == 0:\n",
    "            actor_total_loss = actor_loss\n",
    "        else:\n",
    "            actor_total_loss += actor_loss\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "    actor_loss = actor_loss\n",
    "    actor_loss.backward()\n",
    "    agent.actor.optimizer.step()\n",
    "    if i%20 == 0:\n",
    "        agent.update_critic_target()\n",
    "\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            avg = np.mean(scores[-1000:])\n",
    "            location = \"./\"+run_name+ \"/\"+str(avg)+\"_avg_\"+str(i)+\"_ngames.pt\"\n",
    "            torch.save(agent.actor.state_dict(), location)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode ', i, 'score %.1f' % score,\n",
    "            'average score %.1f' % avg_score, \"    avg single score: \", score/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i+1 for i in range(n_games-100)]\n",
    "plt.plot(scores)\n",
    "location = run_name + \"/\" + run_name\n",
    "plt.savefig(location +\"train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = scores\n",
    "y2 = []\n",
    "mean = lambda x: sum(x)/len(x)\n",
    "for i in range(len(y1)):\n",
    "    if i < 100:\n",
    "        pass\n",
    "    else:\n",
    "        avg = mean(y1[(i-100):i])\n",
    "        y2.append(avg)\n",
    "        \n",
    "plt.plot(y2)\n",
    "#plt.show()\n",
    "plt.savefig(location+\"train_smooth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
